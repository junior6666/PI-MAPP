好的，我将对你的开发文档进行完善和调整，同时对一些细节进行补充和优化，确保文档更加清晰、完整且易于落地。以下是完善后的开发文档：

---

# AI 语音全流程工具开发文档（基于 PySide6）

## 1. 目标
本项目旨在开发一个基于 PySide6 的图形化界面工具，实现以下功能：
1. 抽取视频中的音频。
2. 将音频转录为文字。
3. 使用 GPT 对文字进行润色。
4. 使用原音色将润色后的文字重新合成语音。
5. 若新语音时长大于原时长，自动加速并拼回原视频，保持画面完全同步。
6. 全程通过图形化界面操作。

## 2. 整体架构
```plaintext
┌---------------┐   1. 抽取音轨   ┌---------------┐
│  Input Video  │---------------> │  audio.wav    │
└---------------┘                 └------┬--------┘
                                         │
                                         │2. ASR
                                         ↓
                              ┌---------------------┐
                              │  raw_text.txt       │
                              └---------┬-----------┘
                                        │3. GPT 优化
                                        ↓
                              ┌---------------------┐
                              │  polished.txt       │
                              └---------┬-----------┘
                                        │4. TTS（原音色克隆）
                                        ↓
                              ┌---------------------┐
                              │  new.wav            │
                              └---------┬-----------┘
                                        │5. 时长对齐
                                        ↓
┌---------------┐   6. 合并   ┌---------------------┐
│ Output Video  │<------------│ final_video.mp4     │
└---------------┘             └---------------------┘
```

## 3. 环境依赖
### Python 版本
- Python ≥ 3.9

### 环境配置
- PyTorch GPU（可选，用于加速 TTS 模型）
- FFmpeg（需安装并添加到系统 PATH）

### Python 包依赖
```plaintext
PySide6==6.7.2
openai-whisper
openai>=1.0
ffmpeg-python
pydub
numpy
```

### 音色克隆服务
- **方案 A（推荐）**：Microsoft Azure Neural TTS「Personal Voice」或 ElevenLabs API（支持 30 秒样本克隆）
- **方案 B（本地）**：coqui-ai/TTS + YourTTS（需 GPU）

**注意**：文档中以「Azure Personal Voice」为例，代码中预留接口，可一键替换为其他服务。

## 4. 模块设计
```plaintext
project/
 ├─ app.py               # PySide6 主窗口
 ├─ core/
 │   ├─ audio_extract.py # 音频抽取模块
 │   ├─ asr.py           # ASR（语音转文字）模块
 │   ├─ gpt_polisher.py  # GPT 文字润色模块
 │   ├─ tts_clone.py     # TTS（音色克隆）模块
 │   ├─ align_and_mux.py # 时长对齐与视频合成模块
 │   └─ utils.py         # 辅助工具函数
 ├─ ui/
 │   ├─ main_window.ui   # Qt Designer 文件
 │   └─ generated/       # pyside6-uic 生成的 Python 文件
 ├─ resources/           # 资源文件
 └─ requirements.txt     # 依赖文件
```

## 5. 关键代码实现

### 5.1 音频抽取（audio_extract.py）
```python
import ffmpeg
from pathlib import Path

def extract(video: Path, out_wav: Path):
    """
    从视频中抽取音频并保存为 WAV 格式。
    :param video: 输入视频文件路径
    :param out_wav: 输出音频文件路径
    """
    (
        ffmpeg
        .input(str(video))
        .output(str(out_wav), acodec='pcm_s16le', ac=1, ar='16000')
        .overwrite_output()
        .run(quiet=True)
    )
```

### 5.2 Whisper 转文字（asr.py）
```python
import whisper
import pathlib
import json

def transcribe(wav_path: pathlib.Path) -> str:
    """
    使用 Whisper 模型将音频转录为文字。
    :param wav_path: 输入音频文件路径
    :return: 转录后的文字
    """
    model = whisper.load_model("base")
    result = model.transcribe(str(wav_path), language="zh", word_timestamps=True)
    # 保存含时间戳的 JSON 文件，方便后续对齐
    (wav_path.parent / "raw.json").write_text(json.dumps(result, ensure_ascii=False, indent=2))
    return result["text"]
```

### 5.3 GPT 润色（gpt_polisher.py）
```python
import requests

API_URL = "http://127.0.0.1:11434/api/chat"  

def optimize_text(text: str, style="简洁,符合B站风格") -> str:
    """
    使用 GPT 模型对文字进行润色。
    :param text: 原始文字
    :param style: 润色风格
    :return: 润色后的文字
    """
    prompt = f"请把下面这段文字优化为{style}风格，只返回结果：\n{text}"
    payload = {
        "model": "deepseek-r1:7b",
        "messages": [{"role": "user", "content": prompt}],
        "stream": False
    }
    try:
        resp = requests.post(API_URL, json=payload, timeout=60)
        resp.raise_for_status()  # 检查请求是否成功
        return resp.json()["message"]["content"].strip()
    except requests.RequestException as e:
        print(f"请求 GPT API 时出错：{e}")
        return text  # 如果出错，返回原始文字

# 示例
if __name__ == "__main__":
    raw = "这个产品具有极其出色的性能表现，能够非常好地满足广大用户在日常使用过程中的各种需求。"
    print(optimize_text(raw, style="精炼"))
```

**注意**：如果无法访问 `http://127.0.0.1:11434/api/chat`，请检查链接的合法性或网络连接，或更换为其他可用的 GPT API。

### 5.4 音色克隆 TTS（tts_clone.py）
**方案 A（Azure Personal Voice）**
```python
from azure.cognitiveservices.speech import SpeechConfig, SpeechSynthesizer, AudioConfig

def synthesize(text: str, voice_name: str, output_wav: str):
    """
    使用 Azure Neural TTS 合成语音。
    :param text: 输入文字
    :param voice_name: 音色名称
    :param output_wav: 输出音频文件路径
    """
    speech_config = SpeechConfig(subscription="YOUR_AZURE_SUBSCRIPTION_KEY", region="YOUR_AZURE_REGION")
    audio_config = AudioConfig(filename=output_wav)
    synthesizer = SpeechSynthesizer(speech_config=speech_config, audio_config=audio_config)
    synthesizer.speak_text_async(text).get()
```

**方案 B（Coqui TTS）**
```python
from TTS.api import TTS

def synthesize(text: str, speaker_wav: str, output_wav: str):
    """
    使用 Coqui TTS 合成语音。
    :param text: 输入文字
    :param speaker_wav: 参考音频文件路径
    :param output_wav: 输出音频文件路径
    """
    tts = TTS("tts_models/multilingual/multi-dataset/xtts_v2")
    tts.tts_to_file(text=text, speaker_wav=speaker_wav, language="en", file_path=output_wav)
```

### 5.5 时长对齐 & 视频合成（align_and_mux.py）
```python
import ffmpeg
import pathlib
import math

def mux_aligned(video_in: pathlib.Path, new_audio: pathlib.Path, output: pathlib.Path):
    """
    对齐音频时长并合并到视频中。
    :param video_in: 输入视频文件路径
    :param new_audio: 新音频文件路径
    :param output: 输出视频文件路径
    """
    probe_v = ffmpeg.probe(video_in)
    probe_a = ffmpeg.probe(new_audio)
    dur_v = float(probe_v['format']['duration'])
    dur_a = float(probe_a['format']['duration'])

    # 若音频更长，计算速度因子
    tempo = dur_a / dur_v if dur_a > dur_v else 1.0

    audio = ffmpeg.input(str(new_audio))
    if tempo != 1.0:
        audio = audio.filter('atempo', tempo)

    video = ffmpeg.input(str(video_in))
    out = ffmpeg.output(video.video, audio, str(output),
                        vcodec='copy', acodec='aac', shortest=None)
    out.overwrite_output().run(quiet=True)
```

## 6. PySide6 主界面（app.py）
### 6.1 界面设计
使用 Qt Designer 设计 `main_window.ui`，包含以下控件：
- QProgressBar：显示进度
- QLabel：显示状态信息
- QPushButton：触发流程
- QTextEdit：显示日志或文本
- QFileDialog：选择视频文件

### 6.2 核心槽函数
```python
from PySide6.QtWidgets import QMainWindow, QApplication
from PySide6.QtCore import Qt
from PySide6.QtUiTools import QUiLoader
from pathlib import Path

class MainWindow(QMainWindow):
    def __init__(self):
        super().__init__()
        self.ui = QUiLoader().load("ui/main_window.ui", self)
        self.ui.btn_start.clicked.connect(self.run_pipeline)

    def run_pipeline(self):
        video = Path(self.ui.line_video.text())
        out_dir = video.parent / f"{video.stem}_ai"
        out_dir.mkdir(exist_ok=True)

        steps = [
            ("提取音频", lambda: extract(video, out_dir / "audio.wav")),
            ("语音识别", lambda: setattr(self, "raw_text", transcribe(out_dir / "audio.wav"))),
            ("GPT 优化", lambda: setattr(self, "polished", optimize_text(self.raw_text, "精炼"))),
            ("保存字幕", lambda: (out_dir / "polished.txt").write_text(self.polished, encoding="utf-8")),
            ("TTS 合成", lambda: synthesize(self.polished, "zh-CN-YunfengNeural", out_dir / "new.wav")),
            ("对齐合并", lambda: mux_aligned(video, out_dir / "new.wav", out_dir / "final.mp4")),
        ]
        for name, func in steps:
            self.ui.statusBar.showMessage(name)
            func()
            QApplication.processEvents()
        self.ui.statusBar.showMessage("完成！")

# 启动应用
if __name__ == "__main__":
    app = QApplication([])
    window = MainWindow()
    window.show()
    app.exec()
```

### 6.3 打包发布
使用 PyInstaller 打包应用：
```plaintext
pyinstaller app.py --name=AI_Video_Dub --windowed --add-data "ui:ui" --add-data "resources:resources"
```

## 7. 使用流程
1. **安装依赖与环境变量**
   - 安装 Python 包依赖：`pip install -r requirements.txt`
   - 确保 FFmpeg 已安装并添加到系统 PATH

2. **运行**
   - 启动应用：`python app.py`
   - 选择输入视频文件
   - 点击“开始”按钮，等待流程完成

3. **输出**
   - 处理完成的视频文件将保存在输入视频同目录下的 `xxx_ai/final.mp4`

## 8. 可扩展点
1. **多说话人分离**
   - 使用 `pyannote-audio` 对音频进行说话人分段，然后对每个说话人分别进行音色克隆。
2. **实时预览**
   - 在 TTS 合成步骤中加入 `QMediaPlayer`，实时播放合成的语音。
3. **字幕样式**
   - 使用 Whisper 时间戳生成 `.srt` 字幕文件，并将其烧录到视频画面中。
4. **GPU 加速**
   - 将 TTS 模型切换为本地运行的 XTTS v2，利用 GPU 加速推理，提升处理速度 4-6 倍。
5. **一键批量处理**
   - 使用 `QThreadPool` 并行处理多个视频文件，实现批量操作。

---

以上是完善后的开发文档。希望这份文档能够帮助你更好地理解和实现项目目标。如果有任何问题或需要进一步优化的地方，请随时告诉我。